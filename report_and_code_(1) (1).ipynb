{"cells":[{"cell_type":"markdown","source":["**BBM409 Machine Learning Lab. Assignment 3 Code and Report**\n","\n","In this assignment, we were given a dataset of mails which were labeled as spam and ham mails and we were asked to create a naive bayes model which would classify the given mail. To achieve this we have used bag of words model to get the features as bigrams and unigrams using the CountVectorizer. \n","After the classification part, we were asked to analyse the words that has more affection on the classification with use of TF-IDF and reimplement the naive bayes part, nextly, we were asked to find non-stopwords given the stopwords and lastly, we were asked to analyse the effect of the stopwords and calculate the performance metrics."],"metadata":{"id":"uerZ3q6yaLHT"}},{"cell_type":"markdown","metadata":{"id":"C6JBd-ojoa0d"},"source":["# Importing libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"dStu8bDFoWBF","executionInfo":{"status":"ok","timestamp":1638972528270,"user_tz":-180,"elapsed":545,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["import re\n","from math import log\n","\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import classification_report, accuracy_score\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vd1FKCxDzZKh","outputId":"f572cf50-15e2-4728-d498-3797de36a6ac","executionInfo":{"status":"ok","timestamp":1638972528555,"user_tz":-180,"elapsed":5,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["frozenset"]},"metadata":{},"execution_count":2}],"source":["type(ENGLISH_STOP_WORDS)"]},{"cell_type":"markdown","metadata":{"id":"RQvmXmo7ohAd"},"source":["#Reading dataset"]},{"cell_type":"markdown","source":["Reading and Handling the dataset, by removing the punctuations from the mails given."],"metadata":{"id":"s_TbBMAqdF67"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"UbAgVpxAoirA","executionInfo":{"status":"ok","timestamp":1638972528555,"user_tz":-180,"elapsed":4,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["dataset_df = pd.read_csv('emails.csv')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"iMW1CEOIoyFq","outputId":"6ffc6f96-d3e2-44c8-def1-36f115269d6c","executionInfo":{"status":"ok","timestamp":1638972528555,"user_tz":-180,"elapsed":4,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>spam</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Subject: naturally irresistible your corporate...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Subject: the stock trading gunslinger  fanny i...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Subject: unbelievable new homes made easy  im ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Subject: 4 color printing special  request add...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Subject: do not have money , get software cds ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  spam\n","0  Subject: naturally irresistible your corporate...     1\n","1  Subject: the stock trading gunslinger  fanny i...     1\n","2  Subject: unbelievable new homes made easy  im ...     1\n","3  Subject: 4 color printing special  request add...     1\n","4  Subject: do not have money , get software cds ...     1"]},"metadata":{},"execution_count":4}],"source":["dataset_df.head()  # visualizing the head of the dataset"]},{"cell_type":"markdown","metadata":{"id":"Th8JyWJwMFpH"},"source":["## Cleaning the dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"XWhbUAKkqprR","executionInfo":{"status":"ok","timestamp":1638972529276,"user_tz":-180,"elapsed":724,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["dataset_df['text'] = dataset_df['text'].str.replace('\\W', ' ')  # Removes punctuation\n","dataset_df['text'] = dataset_df['text'].str.lower()"]},{"cell_type":"markdown","metadata":{"id":"0DaADBY436z4"},"source":["# Part 1: Understanding the data"]},{"cell_type":"markdown","metadata":{"id":"g48tswdZppHf"},"source":["Let's pick three specific keywords that might be useful to understand the data: \n","\n","- please\n","- click\n","- easy\n","\n","In our opinion, \"**please**\" will occur in *ham messages* more, because it's a kind word rather than a word that forces people. And since the keyword \"**click**\" is a word that forces people to do something, it will probably occur more in *spam messages*. For the keyword \"**easy**\", it may be occur more in *spam messages* because it is a word used to tell people about doing something. \n","\n","So let's have a look at their appearances in our dataset and see whether our guess' is true or not!"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"NqwtkuYYIPaj","executionInfo":{"status":"ok","timestamp":1638972529277,"user_tz":-180,"elapsed":5,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["texts = dataset_df['text'].values\n","labels = dataset_df['spam'].values"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"n7MszNS5qkvi","executionInfo":{"status":"ok","timestamp":1638972529277,"user_tz":-180,"elapsed":4,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["def find_count_of_word(word, texts, labels) -> tuple:\n","    \"\"\"Finds count of occurrence of a word in given texts.\n","\n","        :param word: The word that is wanted to find its count\n","        :type word: str\n","        :param texts: Messages to search the word\n","        :type texts: numpy.ndarray\n","        :param labels: Labels of the messages\n","        :type labels: numpy.ndarray\n","        :return: A tuple that contains word's count in ham and spam messages, respectively\n","        \"\"\"\n","    count_in_ham, count_in_spam = 0, 0\n","\n","    for text, label in zip(texts, labels):\n","        if word in text:\n","            if label:\n","                count_in_spam += 1\n","            else:\n","                count_in_ham += 1\n","\n","    return count_in_ham, count_in_spam"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"rqKOntzOVGyA","executionInfo":{"status":"ok","timestamp":1638972529277,"user_tz":-180,"elapsed":4,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["def find_prob_of_word(word, texts, labels) -> tuple:\n","    \"\"\"Finds probability of occurrence of a word in given texts.\n","\n","        :param word: The word that is wanted to find its probability\n","        :type word: str\n","        :param texts: Messages to search the word\n","        :type texts: numpy.ndarray\n","        :param labels: Labels of the messages\n","        :type labels: numpy.ndarray\n","        :return: A tuple that contains word's probability in ham and spam messages, respectively\n","        \"\"\"\n","    count_in_ham, count_in_spam = 0, 0\n","    count_ham_words, count_spam_words = 0, 0\n","\n","    for text, label in zip(texts, labels):\n","        count = text.split().count(word)\n","        if label:\n","            count_in_spam += count\n","            count_spam_words += len(text.split())\n","        else:\n","            count_in_ham += count\n","            count_ham_words += len(text.split())\n","\n","    to_round = len(str(ham_count)) - 1\n","    return count_in_ham, count_in_spam, round(count_in_ham / count_ham_words, to_round), round(count_in_spam / count_spam_words, to_round)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"CdByPst-KJAm","executionInfo":{"status":"ok","timestamp":1638972529278,"user_tz":-180,"elapsed":5,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["# finding label counts to use them in probability calculations\n","ham_count = len(labels[dataset_df.spam == 0])  \n","spam_count = len(labels) - ham_count"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ozl_tVK-sGJY","outputId":"bf371d3c-2ba8-429d-83ee-997ff710afb9","executionInfo":{"status":"ok","timestamp":1638972529278,"user_tz":-180,"elapsed":5,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["#########################\n","for \"please\" keyword:\n","-------Count in ham messages------- | ------Count in spam messages-------\n","               2475                 |                 369                \n","\n","----Probability in ham messages---- | ---Probability in spam messages----\n","               0.568                |                0.27                \n","#########################\n","for \"click\" keyword:\n","-------Count in ham messages------- | ------Count in spam messages-------\n","                159                 |                 336                \n","\n","----Probability in ham messages---- | ---Probability in spam messages----\n","               0.036                |                0.246               \n","#########################\n","for \"easy\" keyword:\n","-------Count in ham messages------- | ------Count in spam messages-------\n","                61                  |                 114                \n","\n","----Probability in ham messages---- | ---Probability in spam messages----\n","               0.014                |                0.083               \n","#########################\n"]}],"source":["for word in ['please', 'click', 'easy']:\n","    count_in_ham, count_in_spam = find_count_of_word(word, texts, labels)\n","\n","    to_round = len(str(ham_count)) - 1\n","    prob_in_ham = round(count_in_ham / ham_count, to_round)\n","    prob_in_spam = round(count_in_spam / spam_count, to_round)\n","    print(\"#\" * 25)\n","    print(f'for \"{word}\" keyword:')\n","    print('{0:-^35} | {1:-^35}'.format('Count in ham messages', 'Count in spam messages'))\n","    print(f'{count_in_ham:^35} | {count_in_spam:^35}')\n","    print()\n","    print('{0:-^35} | {1:-^35}'.format('Probability in ham messages', 'Probability in spam messages'))\n","    print(f'{prob_in_ham:^35} | {prob_in_spam:^35}')\n","print(\"#\" * 25)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZaOlUa5nV1bc","outputId":"8089aa73-b66e-4e7b-9e45-beeb719dda9f","executionInfo":{"status":"ok","timestamp":1638972530070,"user_tz":-180,"elapsed":796,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["#########################\n","for \"please\" keyword:\n","-------Count in ham messages------- | ------Count in spam messages-------\n","               4386                 |                 581                \n","\n","----Probability in ham messages---- | ---Probability in spam messages----\n","               0.004                |                0.002               \n","#########################\n","for \"click\" keyword:\n","-------Count in ham messages------- | ------Count in spam messages-------\n","                200                 |                 531                \n","\n","----Probability in ham messages---- | ---Probability in spam messages----\n","                0.0                 |                0.002               \n","#########################\n","for \"easy\" keyword:\n","-------Count in ham messages------- | ------Count in spam messages-------\n","                62                  |                 154                \n","\n","----Probability in ham messages---- | ---Probability in spam messages----\n","                0.0                 |                0.001               \n","#########################\n"]}],"source":["for word in ['please', 'click', 'easy']:\n","    count_in_ham, count_in_spam, prob_in_ham, prob_in_spam = find_prob_of_word(word, texts, labels)\n","\n","    print(\"#\" * 25)\n","    print(f'for \"{word}\" keyword:')\n","    print('{0:-^35} | {1:-^35}'.format('Count in ham messages', 'Count in spam messages'))\n","    print(f'{count_in_ham:^35} | {count_in_spam:^35}')\n","    print()\n","    print('{0:-^35} | {1:-^35}'.format('Probability in ham messages', 'Probability in spam messages'))\n","    print(f'{prob_in_ham:^35} | {prob_in_spam:^35}')\n","print(\"#\" * 25)"]},{"cell_type":"markdown","metadata":{"id":"JXkw5H-b8Tt5"},"source":["## Statistics\n","||\"please\"|\"click\"|\"easy\"|\n","|------|------|------|------|\n","|Count in ham messages|2475|159|61|\n","|Count in spam messages|369|336|114|\n","|Probability in ham messages|0.568|0.036|0.014|\n","|Probability in spam messages|0.27|0.246|0.083|"]},{"cell_type":"markdown","metadata":{"id":"9jLGpHT-BYLm"},"source":["From this table, we can see that the \"**please**\" keyword is being used in more than half of *ham messages*. On the other hand, the keyword \"**click**\" is being used more often in *spam messages* compared to *ham messages*.\n","\n","And for the other keyword \"**easy**\" is not being used often in both of the message types. However, we can easily say that this word occurs in *spam messages* rather than *ham messages*.\n","\n","Our predictions match up with the probabilities that we have found. Therefore, we can say this dataset is **feasible** for predicting whether a mail is ham or spam by using naïve bayes. And since our dataset is text, and naïve bayes uses laplace smoothing concept to find the probability of words, it should work well."]},{"cell_type":"markdown","metadata":{"id":"dwJW2C1NAZVm"},"source":["# Part 2: Implementing Naive Bayes"]},{"cell_type":"markdown","source":["For this part, we have split the dataset and calculated some equations necessary for following parts.\n","\n","Nextly, we have handled the unigram/bigram parts using countvectorizer, found the BoW of the given dataset, calculated the probabilities that are needed for naive bayes. Within this part we have used try except since some words absence were creating bias and handled the bias created by 0 value.\n","\n","For prediction part, we have cleaned text first and calculated log probabilities to handle numerical underflow and took the sum of the probabilities.\n","\n","Lastly, driver part calls the prediction part given the unigram/bigram parts done and calculates the performance overall for both parts."],"metadata":{"id":"jL1wnRIaeeqz"}},{"cell_type":"markdown","metadata":{"id":"Ry2vdKs3LqEi"},"source":["## Splitting the dataset, train set and some calculations"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"LXbQJ14bkELm","executionInfo":{"status":"ok","timestamp":1638972530070,"user_tz":-180,"elapsed":2,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["train_set_df, test_set_df = train_test_split(dataset_df, test_size=0.2, random_state=4)\n","\n","X_test = test_set_df['text'].values\n","y_test = test_set_df['spam'].values\n","\n","# splitting train set as spam and ham\n","spam_mails_df = train_set_df[train_set_df['spam'] == 1]  \n","ham_mails_df = train_set_df[train_set_df['spam'] == 0]\n","\n","# getting values as numpy arrays\n","X_train_spam = spam_mails_df['text'].values\n","X_train_ham = ham_mails_df['text'].values\n","\n","# some variables to use later\n","len_train = len(train_set_df)\n","spam_count = len(X_train_spam)\n","ham_count = len(X_train_ham)\n","p_spam = spam_count / len_train\n","p_ham = ham_count / len_train\n","\n","# laplace smoothing parameter\n","alpha = 1"]},{"cell_type":"markdown","metadata":{"id":"HRTHZ8wPIka0"},"source":["## Unigram part"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"ZUbOIJhT6e_w","executionInfo":{"status":"ok","timestamp":1638972531309,"user_tz":-180,"elapsed":1240,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["spam_vectorizer_unigram = CountVectorizer()\n","spam_bow_unigram = spam_vectorizer_unigram.fit_transform(X_train_spam)\n","spam_vocab_unigram = spam_vectorizer_unigram.get_feature_names_out()\n","\n","ham_vectorizer_unigram = CountVectorizer()\n","ham_bow_unigram = ham_vectorizer_unigram.fit_transform(X_train_ham)\n","ham_vocab_unigram = ham_vectorizer_unigram.get_feature_names_out()\n","\n","vocab_unigram = list(set(spam_vocab_unigram.tolist() + ham_vocab_unigram.tolist()))\n","\n","n_spam_vocab_unigram = len(spam_vocab_unigram)\n","n_ham_vocab_unigram = len(ham_vocab_unigram)\n","n_vocab_unigram = len(vocab_unigram)\n","\n","n_spam = spam_bow_unigram.sum()  # count of words in spam messages\n","\n","n_ham = ham_bow_unigram.sum()  # count of words in ham messages\n","\n","# initialization of words' probabilities\n","wordpbs_spam_unigram = {unique_word:0 for unique_word in vocab_unigram}\n","wordpbs_ham_unigram = {unique_word:0 for unique_word in vocab_unigram}\n","\n","spam_sum = np.sum(spam_bow_unigram, axis=0)  # sum of all unique words' counts, separately\n","ham_sum = np.sum(ham_bow_unigram, axis=0)\n","\n","# calculation of words' probabilities\n","for i in range(n_vocab_unigram):\n","    word = vocab_unigram[i]\n","    try:  # if there is a KeyError, it means the count of the given word is 0\n","        ind = spam_vectorizer_unigram.vocabulary_[word]  # to find the count of the word\n","        n_word_given_spam = spam_sum[0, ind]\n","    except KeyError:\n","        n_word_given_spam = 0\n","    p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha * n_vocab_unigram)\n","    wordpbs_spam_unigram[word] = p_word_given_spam\n","\n","    try:  # same try-except logic in the above one\n","        ind = ham_vectorizer_unigram.vocabulary_[word]\n","        n_word_given_ham = ham_sum[0, ind]\n","    except KeyError:\n","        n_word_given_ham = 0\n","    p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha * n_vocab_unigram)\n","    wordpbs_ham_unigram[word] = p_word_given_ham"]},{"cell_type":"markdown","metadata":{"id":"bY2Y58eQIqyd"},"source":["## Bigram part"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"SntAqfNwVB2u","executionInfo":{"status":"ok","timestamp":1638972535998,"user_tz":-180,"elapsed":4691,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["spam_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n","spam_bow = spam_vectorizer.fit_transform(X_train_spam)\n","spam_vocab_bigram = spam_vectorizer.get_feature_names_out()\n","\n","ham_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n","ham_bow = ham_vectorizer.fit_transform(X_train_ham)\n","ham_vocab_bigram = ham_vectorizer.get_feature_names_out()\n","\n","vocab_bigram = list(set(spam_vocab_bigram.tolist() + ham_vocab_bigram.tolist()))\n","\n","n_spam_vocab_bigram = len(spam_vocab_bigram)\n","n_ham_vocab_bigram = len(ham_vocab_bigram)\n","n_vocab_bigram = len(vocab_bigram)\n","\n","n_spam = spam_bow.sum().sum()\n","\n","n_ham = ham_bow.sum().sum()\n","\n","# initialization of words' probabilities\n","wordpbs_spam_bigram = {unique_word:0 for unique_word in vocab_bigram}\n","wordpbs_ham_bigram = {unique_word:0 for unique_word in vocab_bigram}\n","\n","spam_sum = np.sum(spam_bow, axis=0)\n","ham_sum = np.sum(ham_bow, axis=0)\n","\n","# calculation of words' probabilities\n","for i in range(n_vocab_bigram):\n","    word = vocab_bigram[i]\n","    try:\n","        ind = spam_vectorizer.vocabulary_[word]\n","        n_word_given_spam = spam_sum[0, ind]\n","    except KeyError:\n","        n_word_given_spam = 0\n","    p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha * n_vocab_bigram)\n","    wordpbs_spam_bigram[word] = p_word_given_spam\n","\n","    try:\n","        ind = ham_vectorizer.vocabulary_[word]\n","        n_word_given_ham = ham_sum[0, ind]\n","    except KeyError:\n","        n_word_given_ham = 0\n","    p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha * n_vocab_bigram)\n","    wordpbs_ham_bigram[word] = p_word_given_ham"]},{"cell_type":"markdown","metadata":{"id":"YYVbkrCmIuwc"},"source":["## Prediction functions"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"phNTha67zd59","executionInfo":{"status":"ok","timestamp":1638972535998,"user_tz":-180,"elapsed":9,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["def classify_unigram(message):\n","    # cleaning the given message\n","    message = re.sub('\\W', ' ', message)\n","    message = message.lower().split()\n","\n","    # initialization of probabilities\n","    p_spam_given_message = log(p_spam)\n","    p_ham_given_message = log(p_ham)\n","    \n","    # addition of log probabilities\n","    for word in message:\n","        if word in wordpbs_spam_unigram:\n","            p_spam_given_message += log(wordpbs_spam_unigram[word])\n","        if word in wordpbs_ham_unigram:\n","            p_ham_given_message += log(wordpbs_ham_unigram[word])\n","\n","    if p_ham_given_message > p_spam_given_message:\n","        return 0\n","    elif p_spam_given_message > p_ham_given_message:\n","        return 1\n","    else:\n","        return None"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"m4NigAjw-pSu","executionInfo":{"status":"ok","timestamp":1638972535998,"user_tz":-180,"elapsed":8,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["def classify_bigram(message):\n","    message = re.sub('\\W', ' ', message)\n","    message = message.lower().split()\n","\n","    p_spam_given_message = log(p_spam)\n","    p_ham_given_message = log(p_ham)\n","    \n","    for i in range(len(message) - 1):\n","        word = ' '.join(message[i:i + 2])\n","        if word in wordpbs_spam_bigram:\n","            p_spam_given_message += log(wordpbs_spam_bigram[word])\n","        if word in wordpbs_ham_bigram:\n","            p_ham_given_message += log(wordpbs_ham_bigram[word])\n","\n","    if p_ham_given_message > p_spam_given_message:\n","        return 0\n","    elif p_spam_given_message > p_ham_given_message:\n","        return 1\n","    else:\n","        return None"]},{"cell_type":"markdown","metadata":{"id":"fZpqPAiEIyra"},"source":["## Driver program and calculation of performance metrics"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"oaYHLlP-UuCG","executionInfo":{"status":"ok","timestamp":1638972535998,"user_tz":-180,"elapsed":8,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["def calculate_performance(predictions, y_test):\n","    print(f'Accuracy: {accuracy_score(y_test, predictions)}')\n","    print(f'Precision: {precision_score(y_test, predictions)}')\n","    print(f'Recall: {recall_score(y_test, predictions)}')\n","    print(f'F1_score: {f1_score(y_test, predictions)}')"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JMslSIo3cK9s","outputId":"e5640a46-f449-46de-ded0-c67b3c7d6ca0","executionInfo":{"status":"ok","timestamp":1638972537092,"user_tz":-180,"elapsed":1102,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Unigram performance:\n","Accuracy: 0.993891797556719\n","Precision: 0.9889705882352942\n","Recall: 0.9853479853479854\n","F1_score: 0.9871559633027523\n","\n","Bigram performance:\n","Accuracy: 0.9790575916230366\n","Precision: 0.9844357976653697\n","Recall: 0.9267399267399268\n","F1_score: 0.9547169811320755\n"]}],"source":["predictions_unigram = []\n","predictions_bigram = []\n","\n","for text in X_test:\n","    predictions_unigram.append(classify_unigram(text))\n","    predictions_bigram.append(classify_bigram(text))\n","\n","print('Unigram performance:')\n","calculate_performance(predictions_unigram, y_test)\n","print('\\nBigram performance:')\n","calculate_performance(predictions_bigram, y_test)"]},{"cell_type":"markdown","metadata":{"id":"Yg4mD1nrSEHf"},"source":["# Part 3"]},{"cell_type":"markdown","source":["In this part we have used TF-IDF vectorizer to get the top and bottom words to find the words presence/absence most strongly affects the prediction model. After that we have implemented the part 2 for this new vocabulary we have found by, again calculating the probabilities for these and classification of them for both the unigram and the bigram model.\n","\n","For the stopwords part we have, again, used TF-IDF vectorizer and found the non-stopwords with the use of the given stopwords list.\n","p.s. For this part, we did not include the word \"subject\" since it was not a word that was sent by the user.\n","\n","Lastly for the analysis of the stopwords part, we have subtracted the given stopwords from our mails dataset and then created our model with these data just like the previous parts."],"metadata":{"id":"mCsG0W5vG23n"}},{"cell_type":"markdown","metadata":{"id":"VB-hgor0SLXK"},"source":["## Analyzing effect of the words on prediction"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"xQVANXnKkovD","executionInfo":{"status":"ok","timestamp":1638972537092,"user_tz":-180,"elapsed":2,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["def extract_n_words_top(n, vocab, idfs):\n","    sorted_idfs_indices = np.argsort(idfs)\n","\n","    words_idfs = np.empty((n - 1, 2), dtype=object)\n","\n","    # since 'subject' is not a word written by sender, and every mail contains it, we skip it\n","    for i in range(1, len(sorted_idfs_indices)):\n","        if i >= n:\n","            break\n","\n","        ind = sorted_idfs_indices[i]\n","        \n","        words_idfs[i - 1] = np.array([vocab[ind], idfs[ind]])\n","    \n","    return words_idfs"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"yYWsgMqvuKt9","executionInfo":{"status":"ok","timestamp":1638972537092,"user_tz":-180,"elapsed":2,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["def extract_n_words_bottom(n, vocab, idfs):\n","    sorted_idfs_indices = np.argsort(idfs)[::-1]  # reverse sort to extract from bottom\n","    \n","    words_idfs = np.empty((n, 2), dtype=object)\n","\n","    for i, ind in enumerate(sorted_idfs_indices):\n","        if i >= n:\n","            break\n","        \n","        words_idfs[i] = np.array([vocab[ind], idfs[ind]])\n","    \n","    return words_idfs"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1mU8_18OiOAq","outputId":"53635ecd-dee6-4026-bf00-71a21c8088c4","executionInfo":{"status":"ok","timestamp":1638972538185,"user_tz":-180,"elapsed":1095,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["10 words whose presence most strongly predicts that the mail is ham:\n","[['to' '1.07098124935977']\n"," ['the' '1.0871156359690288']\n"," ['and' '1.1999064615693953']\n"," ['for' '1.222928551492548']\n"," ['you' '1.2240040131484557']\n"," ['of' '1.278170872731738']\n"," ['on' '1.3034789719310338']\n"," ['in' '1.3062011202190185']\n"," ['is' '1.358560731403664']\n"," ['have' '1.4392875740152884']]\n","\n","10 words whose absence most strongly predicts that the mail is ham:\n","[['polach' '8.463936604468925']\n"," ['biting' '8.463936604468925']\n"," ['dostac' '8.463936604468925']\n"," ['practicalities' '8.463936604468925']\n"," ['bitter' '8.463936604468925']\n"," ['dosta' '8.463936604468925']\n"," ['dosage' '8.463936604468925']\n"," ['pracami' '8.463936604468925']\n"," ['ipcre' '8.463936604468925']\n"," ['biura' '8.463936604468925']]\n","\n","10 words whose presence most strongly predicts that the mail is spam:\n","[['to' '1.1717932330051086']\n"," ['the' '1.2541861180235987']\n"," ['and' '1.326124499740307']\n"," ['you' '1.3311942190905577']\n"," ['your' '1.3452699473247423']\n"," ['of' '1.354331498002317']\n"," ['for' '1.4009134388934452']\n"," ['is' '1.4454890634821496']\n"," ['in' '1.4817511945956858']\n"," ['we' '1.6913240259984301']]\n","\n","10 words whose absence most strongly predicts that the mail is spam:\n","[['inspectors' '7.306275286948016']\n"," ['logic' '7.306275286948016']\n"," ['logs' '7.306275286948016']\n"," ['burnt' '7.306275286948016']\n"," ['bursary' '7.306275286948016']\n"," ['logistics' '7.306275286948016']\n"," ['bursts' '7.306275286948016']\n"," ['logician' '7.306275286948016']\n"," ['logical' '7.306275286948016']\n"," ['loftiness' '7.306275286948016']]\n"]}],"source":["spam_tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n","spam_tfidf_vectorizer.fit_transform(X_train_spam)\n","spam_vocab_tfidf = spam_tfidf_vectorizer.get_feature_names_out()\n","spam_idfs = spam_tfidf_vectorizer.idf_\n","spam_presence_words_idfs = extract_n_words_top(11, spam_vocab_tfidf, spam_idfs)\n","spam_absence_words_idfs = extract_n_words_bottom(10, spam_vocab_tfidf, spam_idfs)\n","\n","ham_tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n","ham_tfidf_vectorizer.fit_transform(X_train_ham)\n","ham_vocab_tfidf = ham_tfidf_vectorizer.get_feature_names_out()\n","ham_idfs = ham_tfidf_vectorizer.idf_\n","ham_presence_words_idfs = extract_n_words_top(11, ham_vocab_tfidf, ham_idfs)\n","ham_absence_words_idfs = extract_n_words_bottom(10, ham_vocab_tfidf, ham_idfs)\n","\n","print(\"10 words whose presence most strongly predicts that the mail is ham:\")\n","print(ham_presence_words_idfs)\n","print(\"\\n10 words whose absence most strongly predicts that the mail is ham:\")\n","print(ham_absence_words_idfs)\n","print(\"\\n10 words whose presence most strongly predicts that the mail is spam:\")\n","print(spam_presence_words_idfs)\n","print(\"\\n10 words whose absence most strongly predicts that the mail is spam:\")\n","print(spam_absence_words_idfs)"]},{"cell_type":"markdown","metadata":{"id":"3rF9MGTm4CFI"},"source":["### Reimplementation of Part 2"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"ONJmuaWJ76nA","executionInfo":{"status":"ok","timestamp":1638972538186,"user_tz":-180,"elapsed":3,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["specific_vocab_tfidf = list(set(spam_presence_words_idfs[:, 0].tolist() + spam_absence_words_idfs[:, 0].tolist() + ham_presence_words_idfs[:, 0].tolist() + ham_absence_words_idfs[:, 0].tolist()))"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"dcr2bobs0Nwr","executionInfo":{"status":"ok","timestamp":1638972538186,"user_tz":-180,"elapsed":3,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["# initialization of words' probabilities\n","wordpbs_spam_tfidf = {unique_word:0 for unique_word in specific_vocab_tfidf}\n","wordpbs_ham_tfidf = {unique_word:0 for unique_word in specific_vocab_tfidf}\n","\n","max_val = 9999999\n","# calculation of words' probabilities\n","for i in range(len(specific_vocab_tfidf)):\n","    word = specific_vocab_tfidf[i]\n","    try:  # if there is a KeyError, it means the count of the given word is 0\n","        ind = spam_tfidf_vectorizer.vocabulary_[word]  # to find the count of the word\n","        p_word_given_spam = 1 / spam_idfs[ind]\n","    except KeyError:\n","        p_word_given_spam = 1 / max_val\n","\n","    wordpbs_spam_tfidf[word] = p_word_given_spam\n","\n","    try:  # same try-except logic in the above one\n","        ind = ham_tfidf_vectorizer.vocabulary_[word]\n","        p_word_given_ham = 1 / ham_idfs[ind]\n","    except KeyError:\n","        p_word_given_ham = 1 / max_val\n","\n","    wordpbs_ham_tfidf[word] = p_word_given_ham\n","\n","\n","def classify_unigram_tfidf(message):\n","    # cleaning the given message\n","    message = re.sub('\\W', ' ', message)\n","    message = message.lower().split()\n","\n","    # initialization of probabilities\n","    p_spam_given_message = log(p_spam)\n","    p_ham_given_message = log(p_ham)\n","    \n","    # addition of log probabilities\n","    for word in message:\n","        if word in wordpbs_spam_tfidf:\n","            p_spam_given_message += log(wordpbs_spam_tfidf[word])\n","        if word in wordpbs_ham_tfidf:\n","            p_ham_given_message += log(wordpbs_ham_tfidf[word])\n","\n","    if p_ham_given_message > p_spam_given_message:\n","        return 0\n","    elif p_spam_given_message > p_ham_given_message:\n","        return 1\n","    else:\n","        return None"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"kRy2PMZQyaxR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638972538463,"user_tz":-180,"elapsed":280,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}},"outputId":"4350cb88-5a77-47a7-98fb-4d0ee57db362"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reimplemented bayes performance:\n","Accuracy: 0.7626527050610821\n","Precision: 1.0\n","Recall: 0.003663003663003663\n","F1_score: 0.0072992700729927005\n","\n","               precision    recall  f1-score   support\n","\n","           0       0.76      1.00      0.87       873\n","           1       1.00      0.00      0.01       273\n","\n","    accuracy                           0.76      1146\n","   macro avg       0.88      0.50      0.44      1146\n","weighted avg       0.82      0.76      0.66      1146\n","\n"]}],"source":["predictions = []\n","\n","for text in X_test:\n","    predictions.append(classify_unigram_tfidf(text))\n","\n","print('Reimplemented bayes performance:')\n","calculate_performance(predictions, y_test)\n","print('\\n', classification_report(y_test, predictions))"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"hnNz9gH3pILp","executionInfo":{"status":"ok","timestamp":1638972541690,"user_tz":-180,"elapsed":3227,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["spam_tfidf_vectorizer_bigram = TfidfVectorizer(use_idf=True, ngram_range=(2, 2))\n","spam_tfidf_vectorizer_bigram.fit_transform(X_train_spam)\n","spam_vocab_tfidf_bigram = spam_tfidf_vectorizer_bigram.get_feature_names_out()\n","spam_idfs_bigram = spam_tfidf_vectorizer_bigram.idf_\n","spam_presence_words_idfs_bigram = extract_n_words_top(11, spam_vocab_tfidf_bigram, spam_idfs_bigram)\n","spam_absence_words_idfs_bigram = extract_n_words_bottom(10, spam_vocab_tfidf_bigram, spam_idfs_bigram)\n","\n","ham_tfidf_vectorizer_bigram = TfidfVectorizer(use_idf=True, ngram_range=(2, 2))\n","ham_tfidf_vectorizer_bigram.fit_transform(X_train_ham)\n","ham_vocab_tfidf_bigram = ham_tfidf_vectorizer_bigram.get_feature_names_out()\n","ham_idfs_bigram = ham_tfidf_vectorizer_bigram.idf_\n","ham_presence_words_idfs_bigram = extract_n_words_top(11, ham_vocab_tfidf_bigram, ham_idfs_bigram)\n","ham_absence_words_idfs_bigram = extract_n_words_bottom(10, ham_vocab_tfidf_bigram, ham_idfs_bigram)\n","\n","specific_vocab_tfidf_bigram = list(set(spam_presence_words_idfs_bigram[:, 0].tolist() + spam_absence_words_idfs_bigram[:, 0].tolist() + ham_presence_words_idfs_bigram[:, 0].tolist() + ham_absence_words_idfs_bigram[:, 0].tolist()))\n","\n","# initialization of words' probabilities\n","wordpbs_spam_tfidf_bigram = {unique_word:0 for unique_word in specific_vocab_tfidf_bigram}\n","wordpbs_ham_tfidf_bigram = {unique_word:0 for unique_word in specific_vocab_tfidf_bigram}\n","\n","max_val = 9999999\n","# calculation of words' probabilities\n","for i in range(len(specific_vocab_tfidf_bigram)):\n","    word = specific_vocab_tfidf_bigram[i]\n","    try:  # if there is a KeyError, it means the count of the given word is 0\n","        ind = spam_tfidf_vectorizer_bigram.vocabulary_[word]  # to find the count of the word\n","        p_word_given_spam = 1 / spam_idfs_bigram[ind]\n","    except KeyError:\n","        p_word_given_spam = 1 / max_val\n","\n","    wordpbs_spam_tfidf_bigram[word] = p_word_given_spam\n","\n","    try:  # same try-except logic in the above one\n","        ind = ham_tfidf_vectorizer_bigram.vocabulary_[word]\n","        p_word_given_ham = 1 / ham_idfs_bigram[ind]\n","    except KeyError:\n","        p_word_given_ham = 1 / max_val\n","\n","    wordpbs_ham_tfidf_bigram[word] = p_word_given_ham\n","\n","\n","def classify_bigram_tfidf(message):\n","    # cleaning the given message\n","    message = re.sub('\\W', ' ', message)\n","    message = message.lower().split()\n","\n","    # initialization of probabilities\n","    p_spam_given_message = log(p_spam)\n","    p_ham_given_message = log(p_ham)\n","    \n","    # addition of log probabilities\n","    for i in range(len(message) - 1):\n","        word = ' '.join(message[i:i + 2])\n","        if word in wordpbs_spam_tfidf_bigram:\n","            p_spam_given_message += log(wordpbs_spam_tfidf_bigram[word])\n","        if word in wordpbs_ham_tfidf_bigram:\n","            p_ham_given_message += log(wordpbs_ham_tfidf_bigram[word])\n","\n","    if p_ham_given_message > p_spam_given_message:\n","        return 0\n","    elif p_spam_given_message > p_ham_given_message:\n","        return 1\n","    else:\n","        return None"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pKrNPmZeyRvz","outputId":"f0edee15-44f1-4304-89f4-35f29218c629","executionInfo":{"status":"ok","timestamp":1638972541690,"user_tz":-180,"elapsed":9,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Reimplemented bayes performance:\n","Accuracy: 0.7696335078534031\n","Precision: 0.8461538461538461\n","Recall: 0.040293040293040296\n","F1_score: 0.07692307692307693\n","\n","               precision    recall  f1-score   support\n","\n","           0       0.77      1.00      0.87       873\n","           1       0.85      0.04      0.08       273\n","\n","    accuracy                           0.77      1146\n","   macro avg       0.81      0.52      0.47      1146\n","weighted avg       0.79      0.77      0.68      1146\n","\n"]}],"source":["predictions = []\n","\n","for text in X_test:\n","    predictions.append(classify_bigram_tfidf(text))\n","\n","print('Reimplemented bayes performance:')\n","calculate_performance(predictions, y_test)\n","print('\\n', classification_report(y_test, predictions))"]},{"cell_type":"markdown","metadata":{"id":"Hhj7MAAYSOfg"},"source":["## Stopwords"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"V56bIKYl2Emq","executionInfo":{"status":"ok","timestamp":1638972541691,"user_tz":-180,"elapsed":8,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["def extract_n_words_top(n, vocab, idfs, stop_words):\n","    sorted_idfs_indices = np.argsort(idfs)\n","\n","    words_idfs = np.empty((n - 1, 2), dtype=object)\n","    \n","    count = 0\n","    # since 'subject' is not a word written by sender, and every mail contains it, we skip it\n","    for i in range(1, len(sorted_idfs_indices)):\n","        if count > 9:\n","            break\n","\n","        ind = sorted_idfs_indices[i]\n","        \n","        word = vocab[ind]\n","\n","        if word not in stop_words:\n","            words_idfs[count] = np.array([word, idfs[ind]])\n","            count += 1\n","    \n","    return words_idfs"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"gZOTMBs20a7Y","executionInfo":{"status":"ok","timestamp":1638972542890,"user_tz":-180,"elapsed":1207,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["stop_words = list(ENGLISH_STOP_WORDS)\n","\n","spam_tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n","spam_tfidf_vectorizer.fit_transform(X_train_spam)\n","spam_vocab_tfidf = spam_tfidf_vectorizer.get_feature_names_out()\n","spam_idfs = spam_tfidf_vectorizer.idf_\n","spam_presence_words_idfs = extract_n_words_top(11, spam_vocab_tfidf, spam_idfs, stop_words)\n","\n","ham_tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n","ham_tfidf_vectorizer.fit_transform(X_train_ham)\n","ham_vocab_tfidf = ham_tfidf_vectorizer.get_feature_names_out()\n","ham_idfs = ham_tfidf_vectorizer.idf_\n","ham_presence_words_idfs = extract_n_words_top(11, ham_vocab_tfidf, ham_idfs, stop_words)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r-IMm5865ubo","outputId":"24e2bea8-a6f9-4e30-c145-8977f7f12f95","executionInfo":{"status":"ok","timestamp":1638972542890,"user_tz":-180,"elapsed":4,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["10 non-stopwords that most strongly predict that the mail is spam:\n","[['com' '2.113318436057805']\n"," ['http' '2.2468498286823277']\n"," ['information' '2.3826513698413896']\n"," ['just' '2.389950672323001']\n"," ['click' '2.3973036466282602']\n"," ['business' '2.4464628825863435']\n"," ['email' '2.490034130879984']\n"," ['best' '2.490034130879984']\n"," ['new' '2.506361024167412']\n"," ['time' '2.5104847413512745']]\n","\n","10 non-stopwords that most strongly predict that the mail is ham:\n","[['vince' '1.4432991684301153']\n"," ['enron' '1.5310010269564895']\n"," ['cc' '1.6920010486293227']\n"," ['kaminski' '1.8040018645134381']\n"," ['thanks' '1.8888607638693047']\n"," ['2000' '1.9193055931054273']\n"," ['pm' '1.9447893165285293']\n"," ['ect' '1.995461782089683']\n"," ['know' '2.0382575359667925']\n"," ['hou' '2.0578820414757777']]\n"]}],"source":["print('10 non-stopwords that most strongly predict that the mail is spam:')\n","print(spam_presence_words_idfs)\n","print('\\n10 non-stopwords that most strongly predict that the mail is ham:')\n","print(ham_presence_words_idfs)"]},{"cell_type":"markdown","metadata":{"id":"7xPedXI-STey"},"source":["## Analyzing effect of the stopwords"]},{"cell_type":"markdown","source":["**Why might it make sense to remove stop words when interpreting the model?**\n","\n","While using bag of words based methods, such as, countVectorizer or tfidf that works on counts and frequency of the words, removal of the stopwords helps since it lowers the dimensional space and also a few stop words won't affect the analysis. Removing stopwords also helps the model to get the affective words rather than focusing on commonly used words."],"metadata":{"id":"5E1FT4KuNy1T"}},{"cell_type":"markdown","source":["**Why might it make sense to keep stop words?**\n","\n","With the removal of the stopwords, context of the given texts might get omitted, which means that the removal will damage the model by changing the meanings of the text. For example, if a negation word have got removed from a sentence, the context of the sentence will have changed the opposite way. So, while removing the stopwords, we have to be cautious of what is being dropped.\n"],"metadata":{"id":"NekcdKyOOxd9"}},{"cell_type":"code","execution_count":30,"metadata":{"id":"z0bfidx-JCMy","executionInfo":{"status":"ok","timestamp":1638972542890,"user_tz":-180,"elapsed":2,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"outputs":[],"source":["stop_words = list(ENGLISH_STOP_WORDS)"]},{"cell_type":"code","source":["for index,item in enumerate(texts):\n","  words = [word for word in item.split() if word.lower() not in stop_words]\n","  new_sentence = \" \".join(words)\n","  texts[index] = new_sentence"],"metadata":{"id":"Kt_5VC-SQW94","executionInfo":{"status":"ok","timestamp":1638972549499,"user_tz":-180,"elapsed":6611,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["dataset_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"aUwnNzSJWndI","executionInfo":{"status":"ok","timestamp":1638972549500,"user_tz":-180,"elapsed":4,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}},"outputId":"cbd7a345-45b5-451b-ad52-4f79949caa2f"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>spam</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>subject naturally irresistible corporate ident...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>subject stock trading gunslinger fanny merrill...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>subject unbelievable new homes easy im wanting...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>subject 4 color printing special request addit...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>subject money software cds software compatibil...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  spam\n","0  subject naturally irresistible corporate ident...     1\n","1  subject stock trading gunslinger fanny merrill...     1\n","2  subject unbelievable new homes easy im wanting...     1\n","3  subject 4 color printing special request addit...     1\n","4  subject money software cds software compatibil...     1"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["train_set_df, test_set_df = train_test_split(dataset_df, test_size=0.2, random_state=4)\n","\n","X_test = test_set_df['text'].values\n","y_test = test_set_df['spam'].values\n","\n","# splitting train set as spam and ham\n","spam_mails_df = train_set_df[train_set_df['spam'] == 1]  \n","ham_mails_df = train_set_df[train_set_df['spam'] == 0]\n","\n","# getting values as numpy arrays\n","X_train_spam = spam_mails_df['text'].values\n","X_train_ham = ham_mails_df['text'].values\n","\n","# some variables to use later\n","len_train = len(train_set_df)\n","spam_count = len(X_train_spam)\n","ham_count = len(X_train_ham)\n","p_spam = spam_count / len_train\n","p_ham = ham_count / len_train\n","\n","# laplace smoothing parameter\n","alpha = 1"],"metadata":{"id":"aJSjxID0XDpt","executionInfo":{"status":"ok","timestamp":1638972549501,"user_tz":-180,"elapsed":4,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["Unigram - no stopwords"],"metadata":{"id":"0g3FgxgzXbyD"}},{"cell_type":"code","source":["spam_vectorizer_unigram = CountVectorizer()\n","spam_bow_unigram = spam_vectorizer_unigram.fit_transform(X_train_spam)\n","spam_vocab_unigram = spam_vectorizer_unigram.get_feature_names_out()\n","\n","ham_vectorizer_unigram = CountVectorizer()\n","ham_bow_unigram = ham_vectorizer_unigram.fit_transform(X_train_ham)\n","ham_vocab_unigram = ham_vectorizer_unigram.get_feature_names_out()\n","\n","vocab_unigram = list(set(spam_vocab_unigram.tolist() + ham_vocab_unigram.tolist()))\n","\n","n_spam_vocab_unigram = len(spam_vocab_unigram)\n","n_ham_vocab_unigram = len(ham_vocab_unigram)\n","n_vocab_unigram = len(vocab_unigram)\n","\n","n_spam = spam_bow_unigram.sum()  # count of words in spam messages\n","\n","n_ham = ham_bow_unigram.sum()  # count of words in ham messages\n","\n","# initialization of words' probabilities\n","wordpbs_spam_unigram = {unique_word:0 for unique_word in vocab_unigram}\n","wordpbs_ham_unigram = {unique_word:0 for unique_word in vocab_unigram}\n","\n","spam_sum = np.sum(spam_bow_unigram, axis=0)  # sum of all unique words' counts, separately\n","ham_sum = np.sum(ham_bow_unigram, axis=0)\n","\n","# calculation of words' probabilities\n","for i in range(n_vocab_unigram):\n","    word = vocab_unigram[i]\n","    try:  # if there is a KeyError, it means the count of the given word is 0\n","        ind = spam_vectorizer_unigram.vocabulary_[word]  # to find the count of the word\n","        n_word_given_spam = spam_sum[0, ind]\n","    except KeyError:\n","        n_word_given_spam = 0\n","    p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha * n_vocab_unigram)\n","    wordpbs_spam_unigram[word] = p_word_given_spam\n","\n","    try:  # same try-except logic in the above one\n","        ind = ham_vectorizer_unigram.vocabulary_[word]\n","        n_word_given_ham = ham_sum[0, ind]\n","    except KeyError:\n","        n_word_given_ham = 0\n","    p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha * n_vocab_unigram)\n","    wordpbs_ham_unigram[word] = p_word_given_ham"],"metadata":{"id":"0QLSB2P-XUtS","executionInfo":{"status":"ok","timestamp":1638972550471,"user_tz":-180,"elapsed":973,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["Bigram - no stopword"],"metadata":{"id":"JR1br0wiXlMR"}},{"cell_type":"code","source":["spam_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n","spam_bow = spam_vectorizer.fit_transform(X_train_spam)\n","spam_vocab_bigram = spam_vectorizer.get_feature_names_out()\n","\n","ham_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n","ham_bow = ham_vectorizer.fit_transform(X_train_ham)\n","ham_vocab_bigram = ham_vectorizer.get_feature_names_out()\n","\n","vocab_bigram = list(set(spam_vocab_bigram.tolist() + ham_vocab_bigram.tolist()))\n","\n","n_spam_vocab_bigram = len(spam_vocab_bigram)\n","n_ham_vocab_bigram = len(ham_vocab_bigram)\n","n_vocab_bigram = len(vocab_bigram)\n","\n","n_spam = spam_bow.sum().sum()\n","\n","n_ham = ham_bow.sum().sum()\n","\n","# initialization of words' probabilities\n","wordpbs_spam_bigram = {unique_word:0 for unique_word in vocab_bigram}\n","wordpbs_ham_bigram = {unique_word:0 for unique_word in vocab_bigram}\n","\n","spam_sum = np.sum(spam_bow, axis=0)\n","ham_sum = np.sum(ham_bow, axis=0)\n","\n","# calculation of words' probabilities\n","for i in range(n_vocab_bigram):\n","    word = vocab_bigram[i]\n","    try:\n","        ind = spam_vectorizer.vocabulary_[word]\n","        n_word_given_spam = spam_sum[0, ind]\n","    except KeyError:\n","        n_word_given_spam = 0\n","    p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha * n_vocab_bigram)\n","    wordpbs_spam_bigram[word] = p_word_given_spam\n","\n","    try:\n","        ind = ham_vectorizer.vocabulary_[word]\n","        n_word_given_ham = ham_sum[0, ind]\n","    except KeyError:\n","        n_word_given_ham = 0\n","    p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha * n_vocab_bigram)\n","    wordpbs_ham_bigram[word] = p_word_given_ham"],"metadata":{"id":"Ojeg7IMTXkLw","executionInfo":{"status":"ok","timestamp":1638972554560,"user_tz":-180,"elapsed":4091,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["predictions_unigram = []\n","predictions_bigram = []\n","\n","for text in X_test:\n","    predictions_unigram.append(classify_unigram(text))\n","    predictions_bigram.append(classify_bigram(text))\n","\n","print('Unigram performance:')\n","calculate_performance(predictions_unigram, y_test)\n","print('\\nBigram performance:')\n","calculate_performance(predictions_bigram, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gtjk0DI5XoE5","executionInfo":{"status":"ok","timestamp":1638972554974,"user_tz":-180,"elapsed":421,"user":{"displayName":"berkk karaimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_gyZsJ8lvb3oyqQSdV1CWObsrjszqTiqQOA=s64","userId":"09258690240547373448"}},"outputId":"d1e57a06-6db0-4c40-d324-0b2d3a776c69"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Unigram performance:\n","Accuracy: 0.9947643979057592\n","Precision: 0.992619926199262\n","Recall: 0.9853479853479854\n","F1_score: 0.988970588235294\n","\n","Bigram performance:\n","Accuracy: 0.9790575916230366\n","Precision: 0.973384030418251\n","Recall: 0.9377289377289377\n","F1_score: 0.9552238805970149\n"]}]},{"cell_type":"markdown","source":["Since we are using countVectorizer, removing the stopwords helps, because it lowers the dimensional space and also the results of the program has not changed that much with this action taken."],"metadata":{"id":"84FcGTkpYh_c"}},{"cell_type":"markdown","metadata":{"id":"J0TlmiO6Sj3w"},"source":["# Part 4: Calculation and Analysis of Performance Metrics"]},{"cell_type":"markdown","metadata":{"id":"V8dokek8Sxhn"},"source":["||Accuracy|Precision|Recall|F1Score|\n","|------|------|------|------|------|\n","|Unigram|0.994|0.989|0.985|0.987|\n","|Bigram|0.979|0.984|0.927|0.955|\n","|Unigram-wo/stop|0.994|0.992|0.985|0.988|\n","|Bigram-wo/stop|0.979|0.973|0.937|0.955|"]}],"metadata":{"colab":{"collapsed_sections":["C6JBd-ojoa0d","RQvmXmo7ohAd","Th8JyWJwMFpH","0DaADBY436z4","JXkw5H-b8Tt5","dwJW2C1NAZVm","Ry2vdKs3LqEi","HRTHZ8wPIka0","bY2Y58eQIqyd","YYVbkrCmIuwc","VB-hgor0SLXK","3rF9MGTm4CFI","J0TlmiO6Sj3w"],"name":"report_and_code_(1) (1).ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"}},"nbformat":4,"nbformat_minor":0}